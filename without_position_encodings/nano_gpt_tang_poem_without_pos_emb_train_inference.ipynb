{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8196948b-d969-466b-9fd1-35a862539351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import unicodedata\n",
    "\n",
    "#import time\n",
    "\n",
    "\t\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import unicodedata\n",
    "import pickle\n",
    "\n",
    "#load decoding dictionaries\n",
    "with open('itos.pkl', 'rb') as file:\n",
    "    itos = pickle.load(file)\n",
    "\n",
    "\n",
    "#define function that decodes numbers to texts\n",
    "def decode(ids):\n",
    "    text = \"\".join(itos[idx] for idx in ids)\n",
    "    return text\n",
    "\n",
    "    \n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split =='train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "    \n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X, Y = X.to(device), Y.to(device)            \n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_val_loss(model):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch('val')\n",
    "        X, Y = X.to(device), Y.to(device)            \n",
    "        logits, loss = model(X,Y)\n",
    "        losses[k] = loss.item()\n",
    "    val_loss = losses.mean()\n",
    "    model.train()\n",
    "    return val_loss\n",
    "            \n",
    "\n",
    "        \n",
    "class Head(nn.Module):#modified from above so that 'tril' tensor is always on the same device\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v\n",
    "        return out        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                    nn.Linear(n_embed, 4*n_embed),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(4*n_embed, n_embed),\n",
    "                    nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self,n_embed, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // num_heads\n",
    "        self.sa = MultiHeadAttention(num_heads, head_size) #sa = self attention\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa( self.ln1(x) ) #skip/residual connections\n",
    "        x = x + self.ffwd(  self.ln2(x)  )\n",
    "        return x\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        #self.position_embedding_table = nn.Embedding(block_size, n_embed) #delete position embeddings\n",
    "        self.blocks = nn.Sequential(\n",
    "                    *[Block(n_embed, num_heads ) for _ in range(n_layers)],\n",
    "                    nn.LayerNorm(n_embed),\n",
    "        )\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "        tok_emd  = self.token_embedding_table(idx)\n",
    "        # pos_emd = self.position_embedding_table(torch.arange(T, device = device))  #delete position embeddings\n",
    "        # x= tok_emd + pos_emd             #delete position embeddings\n",
    "        x = tok_emd\n",
    "        x = self.blocks(x)\n",
    "        x = self.ffwd(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "    \n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim =1)\n",
    "        return idx\n",
    "        \n",
    "    def generate_one_poem(self):\n",
    "        idx =  torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "        while True:\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim =1)\n",
    "            if idx_next.item() == 1:\n",
    "                break\n",
    "        return idx\n",
    "\n",
    "def find_poem_lengths(poem):\n",
    "    poem_lens = []\n",
    "    longest_poem_pos = None\n",
    "    poem_len_holder = 0\n",
    "    for char in poem:\n",
    "        poem_len_holder += 1\n",
    "        if char == '>':\n",
    "            poem_lens.append(poem_len_holder)\n",
    "            poem_len_holder = 0\n",
    "    return poem_lens\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce65b1f-e0e7-4b43-8b2c-697fa14db32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Load existing model complete.\n",
      "The model has 8147094 trainable parameters.\n",
      "Embeding dimension = 216,\n",
      "Context length = 500,\n",
      "number of heads per layer = 6,\n",
      "number of layers = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiyang/Documents/venv/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(13997)\n",
    "batch_size = 96\n",
    "block_size = 500\n",
    "\n",
    "vocab_size = len(itos)\n",
    "n_embed = 216\n",
    "num_heads = 6\n",
    "dropout = 0.1\n",
    "n_layers= 8\n",
    "eval_iters = 100\n",
    "\n",
    "\n",
    "\n",
    "m = BigramLanguageModel().to(device)\n",
    "num_params = count_parameters(m)\n",
    "\n",
    "model_path = 'nano_tang_poem_without_pos_emb_layer8_context500_nebd216_nhead6.pt' #8147094 trainable parameters.\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    m.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "    print(f\"Load existing model complete.\")\n",
    "else:\n",
    "    print(\"Creat new model weights file\")\n",
    "\n",
    "print(f\"The model has {num_params} trainable parameters.\")\n",
    "print(f\"Embeding dimension = {n_embed},\\nContext length = {block_size},\\nnumber of heads per layer = {num_heads},\\nnumber of layers = {n_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "769bc5bd-f383-4503-85e6-c24b856bbfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed = 0\n",
      "nano_tang_poem_without_pos_emb_layer8_context500_nebd216_nhead6.pt\n",
      "<寒行台驛|秦樓聽警冬征，虞氏即令威。銅與雲光上，玉爐煙景浮。素姿當歲駟，天步佇光輝。>\n",
      "<中秋夕人房，自使|白首荒城陌，風塵頓寂寥。野花迎古村落，日雨傍池塘。近竹風來晝，幽人夢覺遙。松聲猿叫一聲，宿鳥聽翻簫。杳杳滄溟淺，潺潺在檻寬。>\n",
      "<賀李。殷堯湯|上下渚度秋山，繁花別楚鄉。天河浴行槳，槎枿憑將亡。松殿宜陵雨，關山獨鳥翔。萬家當甲第，雙堠似春香。會合齊平嶽，還將白首陽。>\n",
      "<除妻重見宴駑駘二吳興|自是蘭山別，三台誰與雲。連天南面月，待婢禦衣雲。苑戍連雞劍，樓蘭斷柏尊。封侯天寵獻，見沐漢儀文。>\n",
      "<楊柳陌|金履青嵐八兩坡，碧潭紅杏正開。陸機飛蓋寒花退，青榜高花暗霧飛。舊恨洛陽無女妒，暮塵西笑送青衣。芙蓉朵帶綠銀瓶，橫雉飛驚佩繡行。結綺搖歌自舞，秦檀尾乘鸞。香散何時到唐老，玉顏一度徹雙蛾。>\n",
      "<過遠村人居|商郊急雨家村，江上時聞幾村。自惜不知春早，且將池上曲還。>\n",
      "<曲歌行|楊葉依輕絮斜，水邊深夜夜吳家。隴草蕭條斜日月，離人牽斷翠娥花。遼東北寒與愁來，莫使秋風裏酒杯。三月江亭夜月，誰家越女正開花。說著春寒食路遙，燈微雨送殘花只。萬里暮吟吳畔葉，竟陵春雨辟寒衣。>\n",
      "<榮席|美女嬌初動鬢圓，時時調少莫妍嬌。>\n",
      "<醉輕|酌美金罍催滿安，熟沈玳瑁帳長鏗。更堪縹緲繁須足，莫忘公才有社床空。>\n",
      "<句|瘴何賈生泊，愁容不自眠。有帆琴下過，吹管動湖邊。>\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "print(f'Seed = {seed}')\n",
    "print(model_path)\n",
    "m.eval()\n",
    "for _ in range(10):\n",
    "    print(decode(m.generate_one_poem()[0].tolist()))#loss at 4.54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794a5b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78292f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
