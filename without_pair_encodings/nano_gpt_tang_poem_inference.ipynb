{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8196948b-d969-466b-9fd1-35a862539351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import unicodedata\n",
    "import pickle\n",
    "\n",
    "#load decoding dictionaries\n",
    "with open('itos.pkl', 'rb') as file:\n",
    "    itos = pickle.load(file)\n",
    "\n",
    "\n",
    "#define function that decodes numbers to texts\n",
    "def decode(ids):\n",
    "    text = \"\".join(itos[idx] for idx in ids)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split =='train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "    \n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            X, Y = X.to(device), Y.to(device)            \n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_val_loss(model):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch('val')\n",
    "        X, Y = X.to(device), Y.to(device)            \n",
    "        logits, loss = model(X,Y)\n",
    "        losses[k] = loss.item()\n",
    "    val_loss = losses.mean()\n",
    "    model.train()\n",
    "    return val_loss\n",
    "            \n",
    "\n",
    "        \n",
    "class Head(nn.Module):#modified from above so that 'tril' tensor is always on the same device\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v\n",
    "        return out        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                    nn.Linear(n_embed, 4*n_embed),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(4*n_embed, n_embed),\n",
    "                    nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self,n_embed, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // num_heads\n",
    "        self.sa = MultiHeadAttention(num_heads, head_size) #sa = self attention\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa( self.ln1(x) ) #skip/residual connections\n",
    "        x = x + self.ffwd(  self.ln2(x)  )\n",
    "        return x\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "                    *[Block(n_embed, num_heads ) for _ in range(n_layers)],\n",
    "                    nn.LayerNorm(n_embed),\n",
    "        )\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "\n",
    "        B, T = idx.shape\n",
    "        tok_emd  = self.token_embedding_table(idx)\n",
    "        pos_emd = self.position_embedding_table(torch.arange(T, device = device))\n",
    "        x= tok_emd + pos_emd\n",
    "        x = self.blocks(x)\n",
    "        x = self.ffwd(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "    \n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim =1)\n",
    "        return idx\n",
    "        \n",
    "    def generate_one_poem(self):\n",
    "        idx =  torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "        while True:\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim = -1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim =1)\n",
    "            if idx_next.item() == 1:\n",
    "                break\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ce65b1f-e0e7-4b43-8b2c-697fa14db32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Load existing model complete.\n",
      "The model has 6568998 trainable parameters.\n",
      "Embeding dimension = 216,\n",
      "Context length = 500,\n",
      "number of heads per layer = 6,\n",
      "number of layers = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiyang/Documents/venv/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "block_size = 500\n",
    "vocab_size = len(itos)\n",
    "n_embed = 216\n",
    "num_heads = 6\n",
    "n_layers= 5\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "m = BigramLanguageModel().to(device)\n",
    "num_params = count_parameters(m)\n",
    "\n",
    "# model_path = 'nano_tang_poem_layer8_context80_nebd64_nhead4.pt' # 1423780 trainable parameters\n",
    "# model_path = 'nano_tang_poem_layer10_context80_nebd64_nhead4.pt' # 1,523,364 trainable parameters.\n",
    "# model_path = 'nano_tang_poem_layer10_context80_nebd96_nhead8.pt' #2,674,436 trainable parameters\n",
    "# model_path = 'nano_tang_poem_layer6_context500_nebd216_nhead6.pt' #7,131,030 trainable parameters\n",
    "# model_path = 'nano_tang_poem_layer6_context500_nebd252_nhead6.pt' #9,044,034 trainable parameters\n",
    "# model_path = 'nano_tang_poem_layer7_context500_nebd252_nhead6.pt' #9,808,602trainable parameters\n",
    "# model_path = 'nano_tang_poem_layer7_context500_nebd300_nhead6.pt' #13,000,266 trainable parameters\n",
    "# model_path = 'nano_tang_poem_layer12_context500_nebd252_nhead6.pt' #13,631,442 trainable parameters.\n",
    "# model_path = 'nano_tang_poem_layer4_context500_nebd216_nhead6.pt'  #6,006,966 trainable parameters.\n",
    "# model_path = 'nano_tang_poem_layer5_context500_nebd216_nhead6.pt'  #6,568,998 trainable parameters.\n",
    "# for inference:\n",
    "model_path = f'nano_tang_poem_layer{n_layers}_context{block_size}_nebd{n_embed}_nhead{num_heads}.pt'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    m.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))\n",
    "    print(f\"Load existing model complete.\")\n",
    "else:\n",
    "    print(\"Model file does not exist\")\n",
    "\n",
    "print(f\"The model has {num_params} trainable parameters.\")\n",
    "print(f\"Embeding dimension = {n_embed},\\nContext length = {block_size},\\nnumber of heads per layer = {num_heads},\\nnumber of layers = {n_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "769bc5bd-f383-4503-85e6-c24b856bbfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed = 12312221\n",
      "Using model:  nano_tang_poem_layer5_context500_nebd216_nhead6.pt\n",
      "<送靈徹明王府|耿耿霞山裏，蔥林紫氣微。樓臺淩夕氣，樓作度秋暉。餘雨生秋晚，殘蟬向夕稀。離情徒喜遇，雅思滿前飛。>\n",
      "<贈韋指封南歸|片帆臨太白，潮水聚新泉。雪散江雲遠，雲飛楚草連。紫霞雲夢裏，滄海客行前。白日機心靜，青山磬帶寒。仍為碧海客，俱為春州田。>\n",
      "<納思|曉引迸苔機，雲到架蓬重。桃花清淺景，嶰澗綠縈融。乍拂文雲水，低添舞鶴峰。夜涼前後望，宵吹掃還空。經我鶴指鳳，驅馳雉北風。歌謠燠塵俗，飄落響寒風。白露臨寒景，紅霞帶暖空。蕭疏嘯蕭瑟，淅瀝怨生紅。擁砌聯翩動，兼軒罷卷空。早涼身自喜，落日志難窮。歡賞追歡悅，良辰暇志通。豫遊如未得，還會在歌蓬。>\n",
      "<陪旻公白石屏|伏寺請昨日，八天生甑亡。丹梯我在高，世業常在目。已憐江海舟，千年夏江使。想我二十年，從此便堪異。龍才浮雲車，歲暮涼風利。時施轉蓬龍，玉峰生鬼魅。糞土蹋古木，甘心資寒暑。豐萌何其理，疏俗多精縮。空遷嘉辛裏，鑿石忻無歲。下山順春流，餘雪覆深竹。松蘿起枯木，偶與澗穀廣。靈液泛修修，直須上神怪。不知煩世趣，日月成神格。琴沉日虛靜，齋潔如氛昧。仿佛放未至，焚香生所憶。桃源若有人，蹇步遂無事。方隨化城會，此外唯歸趣。>\n",
      "<關中作|早得稽山不發吹，此生唯有舊名時。晚來楊柳連中老，積雪朦朧在小兒。今日幾宵蘭若故，殷勤更賦訪經時。>\n",
      "<狄昭中相公樓歌|滕公閣夏西壇在，插竹燈陰一望清。江路春還小杏禦，村橋斜日碧芙蓉。南庭不作清朝賞，金殿池台便引行。別後吟聲在書牖，酒醒相思倍四更。>\n",
      "<四明宮之詩：草草磯|梨花片葉滿危汀，莫使朝來不倚春。王母莫留牽意處，懶教明日到深春。>\n",
      "<奉和崔相公|帝山推磬警降祥，豐沛尊陪瑞最祥。自昔齊神堯舜合，長長漢轉禦雞場。>\n",
      "<送刑賢|詩中萬首諭，覺酒各流杯。竹影應難扣，松聲只自開。誰知戎客意，別路石樓臺。>\n",
      "<中馬自河|亞尋花下草頭看，百六州深螮蝀寬。草上橋頭蘇子盛，渭中門裏幕中寒。松風割馬朱錢點，松雪粘花半眼看。>\n"
     ]
    }
   ],
   "source": [
    "seed = 12312221\n",
    "torch.manual_seed(seed)\n",
    "print(f'Seed = {seed}')\n",
    "print(f\"Using model:  {model_path}\")\n",
    "m.eval()\n",
    "for _ in range(10):\n",
    "    print(decode(m.generate_one_poem()[0].tolist()))#loss at 4.53"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
