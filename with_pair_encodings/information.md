The pair encoding is at Chinese character level instead of byte pairs. I only encoded pairs of characters and 
did not encode any pair of "character + punctuation".

The best loss is 4.73,  which is worse than single-character tokenization (best loss = 4.53).  I do not understand the reason yet.
